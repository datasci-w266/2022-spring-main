{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Very Basic spaCy Examples\n",
    "\n",
    "spaCy is an open source industrial strength NLP engine that can perform multiple functions out of the box. It strikes a good balance between speed of processing and accuracy of predictions.  It comes with a number of different language models trained on the [OntoNotes5](https://catalog.ldc.upenn.edu/LDC2013T19) data set.  This means that it is already trained to do part of speech tagging, dependency parsing, semantic role labeling, coreference resolution, and entity detection.  It can also be trained to do classification and a number of other tasks in the standard NLP stack.  It is very fast.  It can be a handy way of analyzing some text. Another use is annotating some text to then create a labelled training set that you use to train up your own model independent of spaCy.\n",
    "\n",
    "It has also been pre-trained on multiple languages.  When using it you need to select and load a specific language model.\n",
    "\n",
    "spaCy uses a combination of techniques including embeddings and convolutional neural nets.  There's a video of [Matthew Honnibal](https://spacy.io/universe/project/video-spacys-ner-model) (at 10:00) describing how Spacy performs named entity recogntion.  He describes the architecutal choices he made in adding the functionality.  You can also add your own training on top of the existing training to enhance the model.\n",
    "\n",
    "Take a look at [this page](https://spacy.io/usage) on the spaCy website to see if you can run it on your machine.  It works on Linux, Mac, and Windows and will operate quite well on a machine without a GPU.  Eventhough it uses Cython you can use pip or conda to install a working package that doesn't require you to run a C compiler.\n",
    "\n",
    "In order to use this notebook, you'll need to install spaCy on your machine.  If you're just experimenting, it is a good idea to use something like the virtualenv to install this so it will not adversely affect your existing set up.\n",
    "\n",
    "Here are the steps, repeated from [this spaCy page](https://spacy.io/usage):\n",
    "\n",
    "#### setup is simple when you create a virtual environment\n",
    "1. make virtualenv\n",
    "2. source .env/bin/activate\n",
    "3. pip install -U spacy\n",
    "4. pip install -U spacy-lookups-data\n",
    "\n",
    "#### get the large english model (it comes with pre-trained embeddings)\n",
    "- python -m spacy download en_core_web_lg\n",
    "\n",
    "#### if you haven't already you should also install pandas so you can capture data for subsequent analysis and use\n",
    "- pip install pandas\n",
    "\n",
    "##### you can also make a special kernel in your jupyter notebook so you know you're running the right environment.\n",
    "##### you can create that in your virtualenv\n",
    "\n",
    "###### to create the kernel for your notebook\n",
    "- python -m ipykernel install --user --name spacy3 --display-name \"Python 3 Spacy3\"\n",
    "###### it will tell you it has created the kernel\n",
    "Installed kernelspec spacy3 in /Users/markhb/Library/Jupyter/kernels/spacy3\n",
    "\n",
    "#### if you want to get a specific version of a language model\n",
    "- python -m spacy download en_core_web_sm-2.1.0 --direct\n",
    "\n",
    "#### if you want to get a non-english model e.g. japanese model\n",
    "- python -m spacy download ja_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.3\n",
      "1.2.4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "print(spacy.__version__)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Language Models\n",
    "Make sure you first load a language model. We're selecting English via the large model which gives us access to embeddings.  There are many other options and other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load an english model -- the large model includes word embeddings\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing\n",
    "When you invoke spaCy with some input text it generates a set of objects.  spaCy wants to process \"document\" like objects. This document can be a sentence or can be many sentences.  You provide text and spaCy runs the nlp function which returns a Doc object.  That Doc object contains a list of Token objects each of which is associated with a set of annotations.  Many examples below are just about harvesting the labels associated with each token after the processing of the Document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first word is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u\"This is a sentence.\")\n",
    "\n",
    "print(\"The first word is: \") \n",
    "doc[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9510932235818049"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We're taking advantage of the large model which uses embeddings \n",
    "#This means we can compute the similarity of two sentences and synonymous words have similar embeddings\n",
    "doc1 = nlp(\"How do I adopt a cat?\")\n",
    "doc2 = nlp(\"How do I obtain a pet?\")\n",
    "\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8914506294237656"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Because this is word based (even with the underlying embeddings) and not a sentence based set of embeddings \n",
    "#these semantically similar sentences aren't as similar as you might expect.\n",
    "doc3 = nlp(\"How old are you?\")\n",
    "doc4 = nlp(\"What is your age?\")\n",
    "\n",
    "doc3.similarity(doc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple functions\n",
    "Spacy is able to perform multiple functions that you might expect from an NLP stack.  Here are examples of some of those functions.  These functions include sentence boundary detection, lemmatization, part of speech tagging,\n",
    "rule based matching, dependency parsing, noun phrase detection, and named entity recognition.  All of this functionality is combined under one umbrella."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Boundary Detection\n",
    "\n",
    "Sentence boundary detection is a hard problem because you can't just look for a period. Abbreviations can contain periods.  Often times, but not always, the boundary of a sentence is a period followed by a space or two and then a capital letter.  A well trained classifier can handle the many possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence boundary detection is actually a pretty hard problem.\n",
      " \n",
      "Great advances have been made in the U.S. in the past decade.\n",
      "New neural nets like a CNN can help improve results on this classification task.\n"
     ]
    }
   ],
   "source": [
    "#sentence detection\n",
    "# Given an input block of text, identify where the sentences end.\n",
    "\n",
    "about_text = ('Sentence boundary detection is actually'\n",
    "              ' a pretty hard problem.  Great advances'\n",
    "              ' have been made in the U.S. in the past decade. New neural nets'\n",
    "              ' like a CNN can help improve results on this classification task.')\n",
    "about_doc = nlp(about_text)\n",
    "sentences = list(about_doc.sents)\n",
    "#len(sentences)\n",
    "\n",
    "#now print out the sentences\n",
    "for sentence in sentences:\n",
    "    print (sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is the identification of the root form of a word.  This means that plural nouns are converted to their signular form.  Similarly a verb is converted to its infinitive form.  It can be useful when you want to count up word occurences and want to consolidate different forms of the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We we\n",
      "are be\n",
      "helping help\n",
      "organize organize\n",
      "all all\n",
      "of of\n",
      "the the\n",
      "conference conference\n",
      "papers paper\n",
      "on on\n",
      "Natural Natural\n",
      "Language Language\n",
      "Processing Processing\n",
      "from from\n",
      "all all\n",
      "conferences conference\n",
      ". .\n",
      "We we\n",
      "keep keep\n",
      "clustering cluster\n",
      "the the\n",
      "papers paper\n",
      "in in\n",
      "to to\n",
      "different different\n",
      "sets set\n",
      "of of\n",
      "subdomains subdomain\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "#since each token is a word you can just print the lemma after the word even though it doesn't look great\n",
    "organize_papers_text = ('We are helping organize all of the'\n",
    "    ' conference papers on Natural Language'\n",
    "    ' Processing from all conferences. We keep clustering the papers'\n",
    "    ' in to different sets of subdomains.')\n",
    "organize_papers_doc = nlp(organize_papers_text)\n",
    "\n",
    "#print out each token and its associated lemma\n",
    "for token in organize_papers_doc:\n",
    "    print (token, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging\n",
    "Part of speech tagging can be very valuable.  Tagging words can allow you to quickly distinguish \"things\" from \"actions\" or \"events.\" spaCy has several different tags to display related to part of speech as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence NN NOUN noun, singular or mass\n",
      "boundary NN NOUN noun, singular or mass\n",
      "detection NN NOUN noun, singular or mass\n",
      "is VBZ AUX verb, 3rd person singular present\n",
      "actually RB ADV adverb\n",
      "a DT DET determiner\n",
      "pretty RB ADV adverb\n",
      "hard JJ ADJ adjective (English), other noun-modifier (Chinese)\n",
      "problem NN NOUN noun, singular or mass\n",
      ". . PUNCT punctuation mark, sentence closer\n",
      "  _SP SPACE whitespace\n",
      "Great JJ ADJ adjective (English), other noun-modifier (Chinese)\n",
      "advances NNS NOUN noun, plural\n",
      "have VBP AUX verb, non-3rd person singular present\n",
      "been VBN AUX verb, past participle\n",
      "made VBN VERB verb, past participle\n",
      "in IN ADP conjunction, subordinating or preposition\n",
      "the DT DET determiner\n",
      "U.S. NNP PROPN noun, proper singular\n",
      "in IN ADP conjunction, subordinating or preposition\n",
      "the DT DET determiner\n",
      "past JJ ADJ adjective (English), other noun-modifier (Chinese)\n",
      "decade NN NOUN noun, singular or mass\n",
      ". . PUNCT punctuation mark, sentence closer\n",
      "New JJ ADJ adjective (English), other noun-modifier (Chinese)\n",
      "neural JJ ADJ adjective (English), other noun-modifier (Chinese)\n",
      "nets NNS NOUN noun, plural\n",
      "like IN ADP conjunction, subordinating or preposition\n",
      "a DT DET determiner\n",
      "CNN NNP PROPN noun, proper singular\n",
      "can MD AUX verb, modal auxiliary\n",
      "help VB VERB verb, base form\n",
      "improve VB VERB verb, base form\n",
      "results NNS NOUN noun, plural\n",
      "on IN ADP conjunction, subordinating or preposition\n",
      "this DT DET determiner\n",
      "classification NN NOUN noun, singular or mass\n",
      "task NN NOUN noun, singular or mass\n",
      ". . PUNCT punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "#POS with unpretty print\n",
    "\n",
    "for token in about_doc:\n",
    "    print (token, token.tag_, token.pos_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>pos</th>\n",
       "      <th>explain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boundary</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>detection</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>AUX</td>\n",
       "      <td>verb, 3rd person singular present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>actually</td>\n",
       "      <td>RB</td>\n",
       "      <td>ADV</td>\n",
       "      <td>adverb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pretty</td>\n",
       "      <td>RB</td>\n",
       "      <td>ADV</td>\n",
       "      <td>adverb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hard</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>adjective (English), other noun-modifier (Chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>problem</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation mark, sentence closer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>_SP</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>whitespace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Great</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>adjective (English), other noun-modifier (Chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>advances</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun, plural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>AUX</td>\n",
       "      <td>verb, non-3rd person singular present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>been</td>\n",
       "      <td>VBN</td>\n",
       "      <td>AUX</td>\n",
       "      <td>verb, past participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>made</td>\n",
       "      <td>VBN</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb, past participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>noun, proper singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>past</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>adjective (English), other noun-modifier (Chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>decade</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation mark, sentence closer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>New</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>adjective (English), other noun-modifier (Chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>neural</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>adjective (English), other noun-modifier (Chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nets</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun, plural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>like</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>CNN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>noun, proper singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>can</td>\n",
       "      <td>MD</td>\n",
       "      <td>AUX</td>\n",
       "      <td>verb, modal auxiliary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>help</td>\n",
       "      <td>VB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb, base form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>improve</td>\n",
       "      <td>VB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb, base form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>results</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun, plural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>this</td>\n",
       "      <td>DT</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>classification</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>task</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation mark, sentence closer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text  tag    pos  \\\n",
       "0         Sentence   NN   NOUN   \n",
       "1         boundary   NN   NOUN   \n",
       "2        detection   NN   NOUN   \n",
       "3               is  VBZ    AUX   \n",
       "4         actually   RB    ADV   \n",
       "5                a   DT    DET   \n",
       "6           pretty   RB    ADV   \n",
       "7             hard   JJ    ADJ   \n",
       "8          problem   NN   NOUN   \n",
       "9                .    .  PUNCT   \n",
       "10                  _SP  SPACE   \n",
       "11           Great   JJ    ADJ   \n",
       "12        advances  NNS   NOUN   \n",
       "13            have  VBP    AUX   \n",
       "14            been  VBN    AUX   \n",
       "15            made  VBN   VERB   \n",
       "16              in   IN    ADP   \n",
       "17             the   DT    DET   \n",
       "18            U.S.  NNP  PROPN   \n",
       "19              in   IN    ADP   \n",
       "20             the   DT    DET   \n",
       "21            past   JJ    ADJ   \n",
       "22          decade   NN   NOUN   \n",
       "23               .    .  PUNCT   \n",
       "24             New   JJ    ADJ   \n",
       "25          neural   JJ    ADJ   \n",
       "26            nets  NNS   NOUN   \n",
       "27            like   IN    ADP   \n",
       "28               a   DT    DET   \n",
       "29             CNN  NNP  PROPN   \n",
       "30             can   MD    AUX   \n",
       "31            help   VB   VERB   \n",
       "32         improve   VB   VERB   \n",
       "33         results  NNS   NOUN   \n",
       "34              on   IN    ADP   \n",
       "35            this   DT    DET   \n",
       "36  classification   NN   NOUN   \n",
       "37            task   NN   NOUN   \n",
       "38               .    .  PUNCT   \n",
       "\n",
       "                                              explain  \n",
       "0                              noun, singular or mass  \n",
       "1                              noun, singular or mass  \n",
       "2                              noun, singular or mass  \n",
       "3                   verb, 3rd person singular present  \n",
       "4                                              adverb  \n",
       "5                                          determiner  \n",
       "6                                              adverb  \n",
       "7   adjective (English), other noun-modifier (Chin...  \n",
       "8                              noun, singular or mass  \n",
       "9                   punctuation mark, sentence closer  \n",
       "10                                         whitespace  \n",
       "11  adjective (English), other noun-modifier (Chin...  \n",
       "12                                       noun, plural  \n",
       "13              verb, non-3rd person singular present  \n",
       "14                              verb, past participle  \n",
       "15                              verb, past participle  \n",
       "16          conjunction, subordinating or preposition  \n",
       "17                                         determiner  \n",
       "18                              noun, proper singular  \n",
       "19          conjunction, subordinating or preposition  \n",
       "20                                         determiner  \n",
       "21  adjective (English), other noun-modifier (Chin...  \n",
       "22                             noun, singular or mass  \n",
       "23                  punctuation mark, sentence closer  \n",
       "24  adjective (English), other noun-modifier (Chin...  \n",
       "25  adjective (English), other noun-modifier (Chin...  \n",
       "26                                       noun, plural  \n",
       "27          conjunction, subordinating or preposition  \n",
       "28                                         determiner  \n",
       "29                              noun, proper singular  \n",
       "30                              verb, modal auxiliary  \n",
       "31                                    verb, base form  \n",
       "32                                    verb, base form  \n",
       "33                                       noun, plural  \n",
       "34          conjunction, subordinating or preposition  \n",
       "35                                         determiner  \n",
       "36                             noun, singular or mass  \n",
       "37                             noun, singular or mass  \n",
       "38                  punctuation mark, sentence closer  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POS\n",
    "#capturing the output in a pandas dataframe makes it easier to view\n",
    "dpos = pd.DataFrame()\n",
    "dpos['text'] = [token.text for token in about_doc]\n",
    "dpos['tag'] = [token.tag_ for token in about_doc]\n",
    "dpos['pos'] = [token.pos_ for token in about_doc]\n",
    "dpos['explain'] = [spacy.explain(token.tag_) for token in about_doc]\n",
    "\n",
    "dpos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-Based Matching\n",
    "spaCy offers a rule based matching capability that allows you to construct rules to match strings and extract them.  This works well if you know all the things you're looking for like an unambigious list of your company's product names.  This just picks out the first proper noun followed by another proper noun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Marti Hearst'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rule based matching\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "nnp_doc = nlp('Marti Hearst and Dan Jurafsky studied with Robert Wilensky at UC Berkeley.')\n",
    "\n",
    "def extract_full_name(nlp_doc):\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    matcher.add('FULL_NAME', [pattern])\n",
    "    matches = matcher(nlp_doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        return span.text\n",
    "\n",
    "extract_full_name(nnp_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing\n",
    "spaCy performs dependency parsing right out of the box.  This can be a very handy way of identifying words and the relations between them.  Sometimes those relations fundamentally change the meaning of the word as in the case of negation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students NNS learning nsubj\n",
      "are VBP learning aux\n",
      "learning VBG learning ROOT\n",
      "Natural NNP Language compound\n",
      "Language NNP Processing compound\n",
      "Processing NNP learning dobj\n",
      "in IN learning prep\n",
      "the DT class det\n",
      "W266 CD class compound\n",
      "class NN in pobj\n",
      ". . learning punct\n"
     ]
    }
   ],
   "source": [
    "#dependency parsing\n",
    "w266_text = 'Students are learning Natural Language Processing in the W266 class.'\n",
    "w266_doc = nlp(w266_text)\n",
    "for token in w266_doc:\n",
    "    print (token.text, token.tag_, token.head.text, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students student NOUN NNS nsubj Xxxxx True False\n",
      "are be AUX VBP aux xxx True True\n",
      "learning learn VERB VBG ROOT xxxx True False\n",
      "Natural Natural PROPN NNP compound Xxxxx True False\n",
      "Language Language PROPN NNP compound Xxxxx True False\n",
      "Processing Processing PROPN NNP dobj Xxxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "W266 w266 NUM CD compound Xddd False False\n",
      "class class NOUN NN pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "# more parsing labels - same w266_doc\n",
    "# you can extract many labels for use in downstream processes\n",
    "for token in w266_doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>is_punctuation</th>\n",
       "      <th>is_space</th>\n",
       "      <th>shape</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>head</th>\n",
       "      <th>dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Students</td>\n",
       "      <td>student</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>learning</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>are</td>\n",
       "      <td>be</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>xxx</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBP</td>\n",
       "      <td>learning</td>\n",
       "      <td>aux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>learning</td>\n",
       "      <td>learn</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>learning</td>\n",
       "      <td>ROOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Language</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Language</td>\n",
       "      <td>Language</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Processing</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Processing</td>\n",
       "      <td>Processing</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>learning</td>\n",
       "      <td>dobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>xx</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>learning</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>xxx</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>class</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>W266</td>\n",
       "      <td>w266</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Xddd</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>class</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>class</td>\n",
       "      <td>class</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>in</td>\n",
       "      <td>pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>learning</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text       lemma  is_punctuation  is_space  shape part_of_speech  \\\n",
       "0     Students     student           False     False  Xxxxx           NOUN   \n",
       "1          are          be           False     False    xxx            AUX   \n",
       "2     learning       learn           False     False   xxxx           VERB   \n",
       "3      Natural     Natural           False     False  Xxxxx          PROPN   \n",
       "4     Language    Language           False     False  Xxxxx          PROPN   \n",
       "5   Processing  Processing           False     False  Xxxxx          PROPN   \n",
       "6           in          in           False     False     xx            ADP   \n",
       "7          the         the           False     False    xxx            DET   \n",
       "8         W266        w266           False     False   Xddd            NUM   \n",
       "9        class       class           False     False   xxxx           NOUN   \n",
       "10           .           .            True     False      .          PUNCT   \n",
       "\n",
       "   pos_tag        head       dep  \n",
       "0      NNS    learning     nsubj  \n",
       "1      VBP    learning       aux  \n",
       "2      VBG    learning      ROOT  \n",
       "3      NNP    Language  compound  \n",
       "4      NNP  Processing  compound  \n",
       "5      NNP    learning      dobj  \n",
       "6       IN    learning      prep  \n",
       "7       DT       class       det  \n",
       "8       CD       class  compound  \n",
       "9       NN          in      pobj  \n",
       "10       .    learning     punct  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if you capture the tags in a dataframe you can then perform additional \n",
    "#operations like counting and filtering and searching\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['text'] = [token.text for token in w266_doc]\n",
    "df['lemma'] = [token.lemma_ for token in w266_doc]\n",
    "df['is_punctuation'] = [token.is_punct for token in w266_doc]\n",
    "df['is_space'] = [token.is_space for token in w266_doc]\n",
    "df['shape'] = [token.shape_ for token in w266_doc]\n",
    "df['part_of_speech'] = [token.pos_ for token in w266_doc]\n",
    "df['pos_tag'] = [token.tag_ for token in w266_doc]\n",
    "df['head'] = [token.head.text for token in w266_doc] \n",
    "df['dep'] = [token.dep_ for token in w266_doc]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Phrase Detection\n",
    "spaCy can identify the noun phrases in the input text.  This can be an interesting set of objects to count if you're doing some basic analytics.  If you simply grabbed bi-grams you would introduce a lot of noise and miss some important parts of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students\n",
      "Natural Language Processing\n",
      "the W266 class\n"
     ]
    }
   ],
   "source": [
    "#noun phrase detection\n",
    "\n",
    "for chunk in w266_doc.noun_chunks:\n",
    "    print (chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recogntion\n",
    "spaCy is also trained to do some basic NER out of the box.  It has been trained using OntoNotes5 so you can see the set of entity tags it uses to annotate its content.  If those don't work for you, then you can train spaCy to identify different entities or use a different tag set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marti Hearst 0 12 PERSON People, including fictional\n",
      "Dan Jurafsky 17 29 PERSON People, including fictional\n",
      "Robert Wilensky 43 58 PERSON People, including fictional\n",
      "UC Berkeley 62 73 ORG Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "#NER example\n",
    "for ent in nnp_doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char,\n",
    "        ent.label_, spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVO Triple Extraction example\n",
    "You can leverage the dependency graph to identify subject-verb-object triples.  These can be used to populate a knowledge graph or to extract \"facts\" from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Students POS: NOUN, dep: nsubj\n",
      "Token are POS: AUX, dep: aux\n",
      "Token learning POS: VERB, dep: ROOT\n",
      "Token Natural POS: PROPN, dep: compound\n",
      "Token Language POS: PROPN, dep: compound\n",
      "Token Processing POS: PROPN, dep: dobj\n",
      "Token in POS: ADP, dep: prep\n",
      "Token the POS: DET, dep: det\n",
      "Token W266 POS: NUM, dep: compound\n",
      "Token class POS: NOUN, dep: pobj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "svo triple:, subject: students, verb: learning, attribute: language processing\n"
     ]
    }
   ],
   "source": [
    "#SVO extraction\n",
    "\n",
    "# object and subject constants\n",
    "OBJECT_DEPS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
    "SUBJECT_DEPS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"agent\", \"expl\"}\n",
    "\n",
    "# extract the subject, object and verb from the input\n",
    "def extract_triples(doc):\n",
    "    sub = []\n",
    "    at = []\n",
    "    ve = []\n",
    "    for token in doc:\n",
    "        # is this a verb?\n",
    "        if token.pos_ == \"VERB\":\n",
    "            ve.append(token.text)\n",
    "        # is this the object?\n",
    "        if token.dep_ in OBJECT_DEPS or token.head.dep_ in OBJECT_DEPS:\n",
    "            at.append(token.text)\n",
    "        # is this the subject?\n",
    "        if token.dep_ in SUBJECT_DEPS or token.head.dep_ in SUBJECT_DEPS:\n",
    "            sub.append(token.text)\n",
    "    return \" \".join(sub).strip().lower(), \" \".join(ve).strip().lower(), \" \".join(at).strip().lower()\n",
    "\n",
    "\n",
    "# print out the pos and deps\n",
    "for token in w266_doc:\n",
    "    print(\"Token {} POS: {}, dep: {}\".format(token.text, token.pos_, token.dep_))\n",
    "\n",
    "# process the input information\n",
    "subject, verb, attribute = extract_triples(w266_doc)\n",
    "print(\"svo triple:, subject: {}, verb: {}, attribute: {}\".format(subject, verb, attribute))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Identification\n",
    "\n",
    "spaCy can also be used to identify questions in text.  Question can be yes/no questions like \"Do you...\" or \"Can you...\" or \"Will you...\"  Question can also use a wh- word like who, what, where, when, or how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token What POS: PRON, dep: dobj\n",
      "Token are POS: AUX, dep: aux\n",
      "Token students POS: NOUN, dep: nsubj\n",
      "Token learning POS: VERB, dep: ROOT\n",
      "Token in POS: ADP, dep: prep\n",
      "Token the POS: DET, dep: det\n",
      "Token W266 POS: NUM, dep: compound\n",
      "Token class POS: NOUN, dep: pobj\n",
      "Token ? POS: PUNCT, dep: punct\n",
      "question type: what\n"
     ]
    }
   ],
   "source": [
    "#question identification\n",
    "w266_qtext = 'What are students learning in the W266 class?'\n",
    "#w266_qtext = 'Do students learn natural language processing in the W266 class?'\n",
    "w266_question = nlp(w266_qtext)\n",
    "\n",
    "\n",
    "# tags that define wether the word is wh-\n",
    "WH_WORDS = {\"WP\", \"WP$\", \"WRB\"}\n",
    "\n",
    "\n",
    "# whether the doc is a question, as well as the wh-word if any\n",
    "def is_question(doc):\n",
    "    # is the first token a verb?\n",
    "    if len(doc) > 0 and doc[0].pos_ == \"AUX\":  # covers both auxiliary & modal verbs\n",
    "        return True, \"yes/no question\"\n",
    "    # go over all words\n",
    "    for token in doc:\n",
    "        # is it a wh- word?\n",
    "        if token.tag_ in WH_WORDS:\n",
    "            return True, token.text.lower()\n",
    "    return False, \"\"\n",
    "\n",
    "\n",
    "for token in w266_question:\n",
    "    print(\"Token {} POS: {}, dep: {}\".format(token.text, token.pos_, token.dep_))\n",
    "\n",
    "# test the input statement\n",
    "question, wh_word = is_question(w266_question)\n",
    "print(\"question type: {}\".format(wh_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
